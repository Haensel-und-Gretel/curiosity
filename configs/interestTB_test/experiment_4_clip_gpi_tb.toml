[config]
save_dir = "Experiment4_GPI_clip_tb"
exp_file = "experiments/1d-tmaze.jl"
exp_module_name = "OneDTmazeExperiment"
exp_func_name = "main_experiment"
arg_iter_type = "iter"

[static_args]

steps = 60000
# steps = 200

logger_keys = ["ONEDTMAZEERROR", "ONED_GOAL_VISITATION", "EPISODE_LENGTH", "INTRINSIC_REWARD", "BEHAVIOUR_ACTION_VALUES", "ONEDTMAZEERROR_DPI"]
logger_interval = 100
# Behaviour Items
behaviour_alpha_init = 0.1 # Will be 1/tilings from sweep
behaviour_gamma = 0.90
behaviour_reward_projector = "maze"
exploration_strategy = "epsilon_greedy"
# behaviour_trace = "AccumulatingTraces"
behaviour_lambda = 0.95
behaviour_w_init = 4.0

# Demon Attributes

demon_alpha_init = 0.1 # Will be 1/tilings from sweep
demon_discounts = 0.9
demon_lambda = 0.9
demon_policy_type = "greedy_to_cumulant"
demon_trace = "AccumulatingTraces"
demon_interest_set = "oned_tmaze"

# demon_learner = "SR"
demon_rep = "ideal"
# demon_update = "TB"
# demon_eta = 0.15
# demon_opt = "Auto"

# Environment Config
constant_target= [-10.0,10.0]
distractor = [1.0, 5.0]
drifter = [1.0, 0.1] # drifter is the sqrt(0.01)
env_step_penalty = -0.01


# exploring_starts="beg"

# Agent and Logger
horde_type = "regular"
intrinsic_reward = "weight_change"

use_external_reward = true
random_first_action = false


# num_tilings = 8
# num_tiles = 2


[sweep_args]
seed = "1:30"
cumulant_schedule = ["DrifterDistractor"]
eta = "5.0.^(-4:-1)"
behaviour_opt = ["Auto"]
behaviour_learner = ["GPI"]
behaviour_update = ["TB"]
exploration_param = [0.1]
demon_learner = ["Q", "SR"]
demon_update = ["TB"]
demon_opt = ["Auto"]

# num_tilings = [4, 8]
# num_tiles = [2]
tiling_structure = [[2,8]]
exploring_starts= ["beg"]
behaviour_trace = ["ReplacingTraces"]
env_step_penalty = [-0.01]
alpha_init = [0.01, 0.1, 0.5, 1.0, 2.0]
